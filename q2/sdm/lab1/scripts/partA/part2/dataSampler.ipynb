{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import random\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a sample of 1000 rows of the large dblp_article.csv file. We will keep a subset of the attributes that are relevant to the problem at hand.\n",
    "\n",
    "We remove NAs from the relevant attributes.\n",
    "\n",
    "We remove duplicate articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articleID      0\n",
      "authors        8\n",
      "journal        0\n",
      "number       565\n",
      "pages        562\n",
      "title          0\n",
      "volume         4\n",
      "year           0\n",
      "dtype: int64\n",
      "articleID      0\n",
      "authors        0\n",
      "journal        0\n",
      "number       553\n",
      "pages        550\n",
      "title          0\n",
      "volume         0\n",
      "year           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../../testData/dblp_article.csv', nrows=1000, delimiter=';', header=None)\n",
    "\n",
    "\n",
    "headers = ['articleID', 'authors', 'journal', 'number', 'pages', 'title', 'volume', 'year']\n",
    "headerPos= [0, 1, 15, 22, 23, 29, 33, 34]\n",
    "df = pd.DataFrame(df.iloc[:, headerPos])\n",
    "df.columns = headers\n",
    "\n",
    "df.describe(include='object')\n",
    "print(df.isna().sum())\n",
    "df = df.dropna(subset=['articleID', 'authors', 'volume'])\n",
    "print(df.isna().sum())\n",
    "\n",
    "df.drop_duplicates(['articleID'], inplace=True)\n",
    "\n",
    "# df.to_csv('../testData/sampleArticle.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Period</h2>\n",
    "\n",
    "Calculate random period between 1 and 5 days of a given year. This will be used to calculate the period in the year a conference is held."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_date(year):\n",
    "    # Get first day of that year i.e. 01/01/year\n",
    "    start_date = dt.date(year, 1, 1)\n",
    "    \n",
    "    # Calculate number of days in that year\n",
    "    days_in_year = (dt.date(year + 1, 1, 1) - start_date).days\n",
    "    \n",
    "    # Get the start day of the year. We guarantee that the start day\n",
    "    # will not be closer than 5 days to the end of the year.\n",
    "    # An example value would be day 180 of the year\n",
    "    start_offset = random.randint(0, days_in_year - 5)\n",
    "    \n",
    "    # We calculate the duration of the conference edition in days\n",
    "    end_offset = random.randint(1, 5)\n",
    "    \n",
    "    # We calculate the start date given the offset\n",
    "    start_date = start_date + dt.timedelta(days=start_offset)\n",
    "    \n",
    "    # We calculate the end date given the start date and duration of the conference edition\n",
    "    end_date = start_date + dt.timedelta(days=end_offset)\n",
    "\n",
    "    # We return the start date and end date of the conference edition\n",
    "    return (start_date.strftime('%d-%m-%Y'), end_date.strftime('%d-%m-%Y'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conference Data</h2>\n",
    "\n",
    "We create a dictionary of 5 conferenced for each year from the earliest year in the dataset to 25 years in the future from now.\n",
    "\n",
    "Each year there is a new edition of each of the conferences. Each edition is held in a random city and period of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of years\n",
    "years = np.arange(start=min(df.year), stop=int(dt.datetime.now().year)+25)\n",
    "\n",
    "# List of conference names\n",
    "conference_names = ['ICML', 'NeurIPS', 'AAAI', 'CVPR', 'ACL']\n",
    "\n",
    "# Dictionary to store year data\n",
    "year_data = {}\n",
    "\n",
    "\n",
    "# Loop over each year\n",
    "for edition, year in enumerate(years):\n",
    "    \n",
    "    # Dictionary to store conference data\n",
    "    conference_data = {}\n",
    "    \n",
    "    # Loop over each conference name\n",
    "    for name in conference_names:\n",
    "        # Choose a random number of editions between 1 and 10\n",
    "        num_editions = random.randint(1, 10)\n",
    "        \n",
    "        # Create a dictionary to store edition data\n",
    "        edition_data = {}\n",
    "        \n",
    "        # Choose a random city for the edition\n",
    "        cities = ['San Francisco', 'New York', 'London', 'Paris', 'Tokyo']\n",
    "        city = random.choice(cities)\n",
    "        \n",
    "        # Choose a random start and end date of the edition\n",
    "        start_date, end_date = random_date(year)\n",
    "        \n",
    "        # Add the edition data to the dictionary\n",
    "        edition_data[name + '_' + str(edition+1)] = {'city': city, 'start_date': start_date, \n",
    "                                                'end_date': end_date, 'number': edition+1}\n",
    "            \n",
    "        # Add the conference data to the dictionary\n",
    "        conference_data[name] = edition_data\n",
    "    \n",
    "    year_data[year] = conference_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generate data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximately 50% of the papers will have been presented\n",
    "# in a conference of the year of publication of that paper\n",
    "hasConf = df[np.random.choice(a=[False, True], size=len(df))]['articleID']\n",
    "\n",
    "# Dictionary of topics. Each topic has a set of keywords.\n",
    "# A random number of keywords from only one topic will be assigned to each paper\n",
    "keywords = {'database': ['data management', 'indexing', 'data modeling', 'big data', 'data processing', 'data storage', 'data querying'],\n",
    "            'artificial intelligence': ['machine learning', 'neural networks', 'deep learning', 'natural language processing', 'computer vision', 'reinforcement learning', 'expert systems', 'knowledge representation', 'genetic algorithms', 'bayesian networks'],\n",
    "            'cybersec' : ['cybersecurity', 'network security', 'information security', 'vulnerability', 'penetration testing', 'threat detection', 'malware analysis', 'security policies', 'risk management', 'cybercrime']\n",
    "}\n",
    "\n",
    "abstracts = ['Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla semper lacus sit amet nisl rhoncus tristique ac vel felis.']\n",
    "\n",
    "authors = df['authors'].apply(lambda x: x.split('|'))\n",
    "authors = list(authors.explode())\n",
    "\n",
    "# All papers related to this journal will:\n",
    "# - Have keywords related to the database community\n",
    "# - Be cited by papers belonging only to this journal\n",
    "# This is done to ensure results in all queries of part B and C.\n",
    "specialJournal = df['journal'].values[0]\n",
    "\n",
    "# Initialise citations column to avoid problems\n",
    "df['citations'] = [[] for _ in range(len(df))]\n",
    "\n",
    "# For each article we assign the missing attributes we need:\n",
    "# - Citations:\n",
    "#   - Each paper can have between 0-30 citations\n",
    "#   - Papers cannot cite themselves\n",
    "#   - Papers cannot cite eachother. If Paper1 cites Paper2, Paper2 cannot cite Paper1\n",
    "#   - Papers cannot cite papers from the future. If Paper1 cites Paper2, Paper2 cannot be from 2022 and Paper1 from 1999\n",
    "# - Keywords\n",
    "#   - Each paper contains between 2-6 keywords of one topic/theme\n",
    "# - Corresponding author\n",
    "#   - Each paper has only one corresponding author\n",
    "#   - The first author from df['authors'] will be the corresponding author\n",
    "# - Reviewers\n",
    "#   - Each paper can have 3-5 reviewers\n",
    "#   - An author of the paper cannot review that paper\n",
    "# - Journal ISSNs (id for each of the unique journals)\n",
    "#   - Unique ID for each journal in dataset\n",
    "# - Volume ID\n",
    "#   - Unique ID for each volume in dataset\n",
    "# - Conference\n",
    "#   - A paper may or may not have been presented in a conference. Aprox. 50% papers were presented in conferences\n",
    "# - Edition\n",
    "# - Edition year\n",
    "# - Edition ID\n",
    "# - Edition number\n",
    "# - Edition City (city)\n",
    "# - Edition start date\n",
    "# - Edition end date\n",
    "\n",
    "\n",
    "for articleIndex, article in df.iterrows():\n",
    "    keyword   = random.choice(list(keywords.keys()))\n",
    "    \n",
    "    # This is to guarantee that there will be at least one journal in the graph which\n",
    "    # 90% of its papers are associated to the community of database\n",
    "    # This is done to guarantee that recommender system query returns at least one result.\n",
    "    #\n",
    "    # Papers belonging to the specialJournal\n",
    "    if (df.loc[df['articleID'] == article['articleID'], 'journal'] == specialJournal).all():\n",
    "        citations = random.choices(df.loc[df['journal'] == specialJournal,'articleID'].values, k=random.randint(0, df['journal'].value_counts()[specialJournal]-1))\n",
    "        words = random.choices(keywords['database'], k=random.randint(2, 6))\n",
    "    # Papers not belonging\n",
    "    else: \n",
    "        citations = random.choices(df['articleID'].values, k=random.randint(0, 30))\n",
    "        words     = random.choices(keywords[keyword], k=random.randint(2, 6))\n",
    "\n",
    "    # Remove any repeated citations\n",
    "    citations = list(set(citations))\n",
    "\n",
    "    \n",
    "    # Remove the current paper from citations. A paper cannot cite itself\n",
    "    # Two papers also cannot cite eachother\n",
    "    for citation in citations:\n",
    "        if (citation == article['articleID'] or \n",
    "            article['articleID'] in df.loc[df['articleID'] == citation, 'citations'].values[0] or \n",
    "            article['year'] >= df.loc[df['articleID'] == citation, 'year'].values[0]):\n",
    "            \n",
    "            citations.remove(citation)\n",
    "            \n",
    "\n",
    "    # From the current paper being processed, the first author in the list will be\n",
    "    # the corresponding author\n",
    "    df.loc[df['articleID'] == article['articleID'], 'correspondingAuthor'] = random.choice(list(df[df['articleID'] == article['articleID']]['authors'].str.split('|'))[0])\n",
    "    \n",
    "    # Each article has a random number of keywords of the same theme/topic\n",
    "    df.loc[df['articleID'] == article['articleID'], 'keywords'] = '|'.join(words)\n",
    "\n",
    "    \n",
    "    if len(citations) > 0:\n",
    "        # Each article will cite a random number of other articles\n",
    "        df.at[articleIndex, 'citations'] = citations \n",
    "    \n",
    "    reviewers = random.choices(authors, k=random.randint(3, 5))\n",
    "    if article['articleID'] in hasConf.values:\n",
    "        temp_conference = random.choice(conference_names)\n",
    "        temp_year = int(int(df[df['articleID'] == article['articleID']]['year']))\n",
    "        temp_edition = list(year_data[temp_year][temp_conference].keys())[0]\n",
    "\n",
    "        df.loc[df['articleID'] == article['articleID'], 'edition_year'] = temp_year\n",
    "        df.loc[df['articleID'] == article['articleID'], 'conference'] = temp_conference\n",
    "        df.loc[df['articleID'] == article['articleID'], 'edition_id'] = temp_edition\n",
    "        df.loc[df['articleID'] == article['articleID'], 'edition_number'] = year_data[temp_year][temp_conference][temp_edition]['number']\n",
    "        df.loc[df['articleID'] == article['articleID'], 'city'] = year_data[temp_year][temp_conference][temp_edition]['city']\n",
    "        df.loc[df['articleID'] == article['articleID'], 'start_date'] = year_data[temp_year][temp_conference][temp_edition]['start_date']\n",
    "        df.loc[df['articleID'] == article['articleID'], 'end_date'] = year_data[temp_year][temp_conference][temp_edition]['end_date']\n",
    "\n",
    "    # Remove the author of current article. An author cannot review their own articles\n",
    "    for reviewer in reviewers:\n",
    "        if reviewer in article['authors']: reviewers.remove(reviewer)\n",
    "    \n",
    "    # Each article has a random number of keywords of the same theme/topic\n",
    "    df.loc[df['articleID'] == article['articleID'], 'reviewers'] = '|'.join(reviewers)\n",
    "\n",
    "    # Each paper has a random abstract\n",
    "    df.loc[df['articleID'] == article['articleID'], 'abstract'] = random.choice(abstracts)\n",
    "\n",
    "# Confirm that papers cannot cite eachother. If Paper1 cites Paper2, Paper2 cannot cite Paper1\n",
    "for articleIndex, article in df.iterrows():\n",
    "    citations = article['citations']\n",
    "    for citation in citations:\n",
    "        if (article['articleID'] in df.loc[df['articleID'] == citation, 'citations'].values[0]):\n",
    "            citations.remove(citation)\n",
    "    df.at[articleIndex, 'citations'] = citations\n",
    "\n",
    "# Format citaions to follow the same structure as authors and reviewers\n",
    "# Example: ciation1|citation2|citation3\n",
    "df['citations'] = df['citations'].apply(lambda x: '|'.join(str(i) for i in x))\n",
    "\n",
    "# We generate random unique ids for each of the journals\n",
    "journalNames = np.unique(df['journal'].values)\n",
    "repeat = True\n",
    "while repeat:\n",
    "    journalISSNS = np.random.randint(low=10000000, high=99999999, size=len(journalNames))\n",
    "    repeat = len(journalISSNS) != len(np.unique(journalISSNS))\n",
    "\n",
    "# We assign an issn to each journal and a volume id to each of its volumes\n",
    "for i, journal in enumerate(journalNames):\n",
    "    df.loc[df['journal'] == journal, 'issn'] = str(journalISSNS[i])\n",
    "    df.loc[df['journal'] == journal, 'volume_id'] = str(journalISSNS[i]) + '-' + df[df['journal'] == journal]['volume']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../../testData/sampleArticle.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b1f1339d7c2ab996716129d986c503100b4901e5db18f38c21cac048359f26b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
